{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import string\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "import re\n",
    "from scipy import stats\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import math\n",
    "from scipy.stats import shapiro, mannwhitneyu\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = ['care', 'fairness', 'loyalty', 'authority', 'sanctity']\n",
    "polarities = ['virtue', 'vice']\n",
    "\n",
    "# dictionaries\n",
    "dictionary_paths = {\n",
    "    'mft': '/workspaces/debates_analysis/dictionaries/processed/mft_dictionary.json',\n",
    "    'mfd2': '/workspaces/debates_analysis/dictionaries/processed/mfd2_dictionary.json',\n",
    "    'mfd1': '/workspaces/debates_analysis/dictionaries/processed/mfd1_dictionary.json',\n",
    "    'emfd': '/workspaces/debates_analysis/dictionaries/processed/emfd_dictionary.json',\n",
    "    'ms': '/workspaces/debates_analysis/dictionaries/processed/ms_dictionary.json'\n",
    "}\n",
    "\n",
    "\n",
    "loaded_dictionaries = {}\n",
    "for key, path in dictionary_paths.items():\n",
    "    with open(path, 'r') as file:\n",
    "        loaded_dictionaries[key] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspaces/debates_analysis/debate_transcripts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    pattern = r\"\\[crosstalk \\d{2}:\\d{2}:\\d{2}\\]\"\n",
    "    cleaned_text = re.sub(pattern, \"\", text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "    # Normalize case (except for proper nouns)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    tokens = [word.lower() if tag != 'NNP' and tag != 'NNPS' else word for word, tag in tagged_tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words_dic = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words_dic]\n",
    "\n",
    "    # Remove punctuation and numbers, keeping only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Lemmatize tokens using the appropriate WordNet tag\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(tokens, dictionary):\n",
    "    # Initialize scores\n",
    "    scores = {dimension: {polarity: 0 for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "    tracked = {dimension: {polarity: [] for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "\n",
    "    # Iterate through each token and update scores directly based on occurrence\n",
    "    for token in tokens:\n",
    "        for dimension, polarities in dictionary.items():\n",
    "            for polarity, entries in polarities.items():\n",
    "                # take only strings (words) in tuples\n",
    "                words = [entry if isinstance(entry, str) else entry[0] for entry in entries]\n",
    "\n",
    "                if token in words:\n",
    "                    # Increment score by 1 for each occurrence\n",
    "                    scores[dimension][polarity] += 1\n",
    "                    # Track the token if not already tracked\n",
    "                    if token not in tracked[dimension][polarity]:\n",
    "                        tracked[dimension][polarity].append(token)\n",
    "\n",
    "    # Flatten scores to a vector\n",
    "    vector = []\n",
    "    for dimension in scores.keys():\n",
    "        for polarity in scores[dimension].keys():\n",
    "            vector.append(scores[dimension][polarity])\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dictionary in loaded_dictionaries.items():\n",
    "    df[key] = df['preprocessed_text'].apply(lambda x: score(x, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>debate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>mft</th>\n",
       "      <th>mfd2</th>\n",
       "      <th>mfd1</th>\n",
       "      <th>emfd</th>\n",
       "      <th>ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Moderator</td>\n",
       "      <td>00:01:20</td>\n",
       "      <td>Good evening from the Health Education Campus ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, evening, Health, Education, Campus, Cas...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 2, 2, 0, 0]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 0, 4, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Moderator</td>\n",
       "      <td>00:02:10</td>\n",
       "      <td>This debate is being conducted under health an...</td>\n",
       "      <td>1</td>\n",
       "      <td>[debate, conducted, health, safety, protocol, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 0, 0, 0, 3, 0, 0]</td>\n",
       "      <td>[5, 1, 5, 1, 5, 1, 5, 1, 5, 1]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>00:02:49</td>\n",
       "      <td>How you doing, man?</td>\n",
       "      <td>1</td>\n",
       "      <td>[man]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>00:02:51</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>00:02:51</td>\n",
       "      <td>I’m well.</td>\n",
       "      <td>1</td>\n",
       "      <td>[well]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker      time                                               text  \\\n",
       "0     Moderator  00:01:20  Good evening from the Health Education Campus ...   \n",
       "1     Moderator  00:02:10  This debate is being conducted under health an...   \n",
       "2     Joe Biden  00:02:49                                How you doing, man?   \n",
       "3  Donald Trump  00:02:51                                 How are you doing?   \n",
       "4     Joe Biden  00:02:51                                          I’m well.   \n",
       "\n",
       "   debate                                  preprocessed_text  \\\n",
       "0       1  [good, evening, Health, Education, Campus, Cas...   \n",
       "1       1  [debate, conducted, health, safety, protocol, ...   \n",
       "2       1                                              [man]   \n",
       "3       1                                                 []   \n",
       "4       1                                             [well]   \n",
       "\n",
       "                              mft                            mfd2  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                             mfd1                            emfd  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 2, 2, 0, 0]  [4, 0, 4, 0, 4, 0, 4, 0, 4, 0]   \n",
       "1  [2, 0, 0, 1, 0, 0, 0, 3, 0, 0]  [5, 1, 5, 1, 5, 1, 5, 1, 5, 1]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                               ms  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1301, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
